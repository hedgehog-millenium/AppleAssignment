{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").getOrCreate()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Example_data_for_Apple_BigData_Tasks.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------+-------------------+-----------------+\n",
      "|     category|             product|  userId|          eventTime|        eventType|\n",
      "+-------------+--------------------+--------+-------------------+-----------------+\n",
      "|        books|   Scala for Dummies|user 100|2018-03-01 12:00:00| view description|\n",
      "|        books|   Scala for Dummies|user 100|2018-03-01 12:01:00|             like|\n",
      "|        books|   Scala for Dummies|user 100|2018-03-01 12:01:00|     check status|\n",
      "|        books|    Java for Dummies|user 100|2018-03-01 12:02:00| view description|\n",
      "|        books|    Java for Dummies|user 100|2018-03-01 12:02:00|          dislike|\n",
      "|        books|    Java for Dummies|user 100|2018-03-01 12:02:00|close description|\n",
      "|        books|   Scala for Dummies|user 100|2018-03-01 12:04:00| view description|\n",
      "|        books|   Scala for Dummies|user 100|2018-03-01 12:06:00|    add to bucket|\n",
      "|        books|Sherlock Holmes, ...|user 200|2018-03-01 12:11:00| view description|\n",
      "|        books|Sherlock Holmes, ...|user 200|2018-03-01 12:12:00|     check status|\n",
      "|        books|Sherlock Holmes, ...|user 200|2018-03-01 12:13:00| sign for updates|\n",
      "|        books|    Romeo and Juliet|user 200|2018-03-01 12:15:00| view description|\n",
      "|        books|    Romeo and Juliet|user 200|2018-03-01 12:15:00|     check status|\n",
      "|        books|    Romeo and Juliet|user 200|2018-03-01 12:15:00|    add to bucket|\n",
      "|mobile phones|            iPhone X|user 300|2018-03-01 12:05:00| view description|\n",
      "|mobile phones|            iPhone X|user 300|2018-03-01 12:05:00|    add to bucket|\n",
      "|mobile phones|       iPhone 8 Plus|user 100|2018-03-01 12:06:00| view description|\n",
      "|mobile phones|       iPhone 8 Plus|user 100|2018-03-01 12:07:00|    add to bucket|\n",
      "|mobile phones|            iPhone 8|user 100|2018-03-01 12:08:00| view description|\n",
      "|mobile phones|            iPhone 8|user 100|2018-03-01 12:13:00|          dislike|\n",
      "+-------------+--------------------+--------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns\n",
    "df = df.withColumn(\"eventTime\" ,f.to_timestamp(\"eventTime\", \"MM/dd/yy HH:mm\"))\n",
    "df.createOrReplaceTempView(\"visits\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Enrich with session \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task #1: \n",
    "Enrich incoming data with user sessions. Definition of a session: for each user, it contains consecutive events that belong to a single category  and are not more than 5 minutes away from each other. Output should look like this (session columns are in bold):\n",
    "eventTime, eventType, category, userId, â€¦, sessionId, sessionStartTime, sessionEndTime  \n",
    "Implement it using 1) sql window functions and 2) Spark aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+-------------------+-----------+-------------------+-------------------+\n",
      "|  userId|     category|        eventType|          eventTime|     sessId|      sessStartDate|        sessEndDate|\n",
      "+--------+-------------+-----------------+-------------------+-----------+-------------------+-------------------+\n",
      "|user 100|        books| view description|2018-03-01 12:00:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|             like|2018-03-01 12:01:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|     check status|2018-03-01 12:01:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books| view description|2018-03-01 12:02:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|          dislike|2018-03-01 12:02:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|close description|2018-03-01 12:02:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books| view description|2018-03-01 12:04:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|    add to bucket|2018-03-01 12:06:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 200|    notebooks| view description|2018-03-01 12:15:00| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|\n",
      "|user 200|    notebooks|             like|2018-03-01 12:17:00| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|\n",
      "|user 200|    notebooks|    add to bucket|2018-03-01 12:17:00| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|\n",
      "|user 200|        books| view description|2018-03-01 12:11:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books|     check status|2018-03-01 12:12:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books| sign for updates|2018-03-01 12:13:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books| view description|2018-03-01 12:15:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books|     check status|2018-03-01 12:15:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books|    add to bucket|2018-03-01 12:15:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 300|    notebooks| view description|2018-03-01 12:14:00|-2038065633|2018-03-01 12:14:00|2018-03-01 12:14:00|\n",
      "|user 100|mobile phones| view description|2018-03-01 12:06:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|mobile phones|    add to bucket|2018-03-01 12:07:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|mobile phones| view description|2018-03-01 12:08:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|mobile phones|          dislike|2018-03-01 12:13:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|    notebooks| view description|2018-03-01 12:15:00|  653119526|2018-03-01 12:15:00|2018-03-01 12:16:00|\n",
      "|user 100|    notebooks|     check status|2018-03-01 12:16:00|  653119526|2018-03-01 12:15:00|2018-03-01 12:16:00|\n",
      "|user 300|mobile phones| view description|2018-03-01 12:05:00|  197472950|2018-03-01 12:05:00|2018-03-01 12:05:00|\n",
      "|user 300|mobile phones|    add to bucket|2018-03-01 12:05:00|  197472950|2018-03-01 12:05:00|2018-03-01 12:05:00|\n",
      "|user 300|    notebooks|          dislike|2018-03-01 12:20:00| 1939065348|2018-03-01 12:20:00|2018-03-01 12:20:00|\n",
      "+--------+-------------+-----------------+-------------------+-----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SPARK SQL QUERY\n",
    "# Take LAG() to compare previous eventTime with current . If Diff > 5 new session kicked off\n",
    "# Set -1 to events having no previous event --> Again means begening of new session \n",
    "\n",
    "spark.sql(\n",
    "\"\"\"SELECT *,\n",
    "           MIN(eventTime) OVER (PARTITION BY sessId) as sessStartDate,\n",
    "           MAX(eventTime) OVER (PARTITION BY sessId) as sessEndDate\n",
    "   FROM (\n",
    "       SELECT userId,category, eventType ,eventTime, \n",
    "          IF(\n",
    "              IFNULL(\n",
    "                      (\n",
    "                          CAST(eventTime AS LONG) - \n",
    "                          CAST(LAG(eventTime) OVER (PARTITION BY userId, category ORDER BY eventTime) AS LONG)\n",
    "                      )/60,\n",
    "                      -1\n",
    "                  ) BETWEEN 0 AND 5 ,\n",
    "              FIRST(HASH(CONCAT(userId,category,eventType,eventTime))) OVER (PARTITION BY userId, category ORDER BY eventTime),\n",
    "              HASH(CONCAT(userId,category,eventType,eventTime))\n",
    "\n",
    "          ) as sessId\n",
    "       FROM visits)\n",
    "\"\"\").createOrReplaceTempView(\"visits_with_sess\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM visits_with_sess\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+-------------------+-----------+-------------------+-------------------+\n",
      "|  userId|     category|        eventType|          eventTime|     sessId|      sessStartDate|        sessEndDate|\n",
      "+--------+-------------+-----------------+-------------------+-----------+-------------------+-------------------+\n",
      "|user 100|        books| view description|2018-03-01 12:00:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|             like|2018-03-01 12:01:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|     check status|2018-03-01 12:01:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books| view description|2018-03-01 12:02:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|          dislike|2018-03-01 12:02:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|close description|2018-03-01 12:02:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books| view description|2018-03-01 12:04:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 100|        books|    add to bucket|2018-03-01 12:06:00| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|\n",
      "|user 200|    notebooks| view description|2018-03-01 12:15:00| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|\n",
      "|user 200|    notebooks|             like|2018-03-01 12:17:00| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|\n",
      "|user 200|    notebooks|    add to bucket|2018-03-01 12:17:00| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|\n",
      "|user 200|        books| view description|2018-03-01 12:11:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books|     check status|2018-03-01 12:12:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books| sign for updates|2018-03-01 12:13:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books| view description|2018-03-01 12:15:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books|     check status|2018-03-01 12:15:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 200|        books|    add to bucket|2018-03-01 12:15:00| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|\n",
      "|user 300|    notebooks| view description|2018-03-01 12:14:00|-2038065633|2018-03-01 12:14:00|2018-03-01 12:14:00|\n",
      "|user 100|mobile phones| view description|2018-03-01 12:06:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|mobile phones|    add to bucket|2018-03-01 12:07:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|mobile phones| view description|2018-03-01 12:08:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|mobile phones|          dislike|2018-03-01 12:13:00|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|\n",
      "|user 100|    notebooks| view description|2018-03-01 12:15:00|  653119526|2018-03-01 12:15:00|2018-03-01 12:16:00|\n",
      "|user 100|    notebooks|     check status|2018-03-01 12:16:00|  653119526|2018-03-01 12:15:00|2018-03-01 12:16:00|\n",
      "|user 300|mobile phones| view description|2018-03-01 12:05:00|  197472950|2018-03-01 12:05:00|2018-03-01 12:05:00|\n",
      "|user 300|mobile phones|    add to bucket|2018-03-01 12:05:00|  197472950|2018-03-01 12:05:00|2018-03-01 12:05:00|\n",
      "|user 300|    notebooks|          dislike|2018-03-01 12:20:00| 1939065348|2018-03-01 12:20:00|2018-03-01 12:20:00|\n",
      "+--------+-------------+-----------------+-------------------+-----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Spark aggregation\n",
    "evnt_lag_mins = 5\n",
    "sel_cols = ['userId','category', 'eventType',  'eventTime','sessId','sessStartDate','sessEndDate']\n",
    "w = Window.partitionBy(f.col('userId'),f.col('category')).orderBy(f.col(\"eventTime\"))\n",
    "w_sess = Window.partitionBy(f.col('sessId'))\n",
    "\n",
    "df_w = df.withColumn('prevEvtTS', f.lag('eventTime').over(w))\\\n",
    "         .withColumn(\"lagPrevEvtMin\",(f.col('eventTime').cast(\"long\")-f.col('prevEvtTS').cast(\"long\"))/60).na.fill(-1)\\\n",
    "         .withColumn('hash',f.hash(f.concat(f.col('userId'),f.col('category'),f.col('eventType'),f.col('eventTime'))))\\\n",
    "         .withColumn('sessId',f.when(\n",
    "                                    f.col('lagPrevEvtMin').between(0,evnt_lag_mins),\n",
    "                                    f.first('hash').over(w))\n",
    "                                .otherwise(f.col('hash')))\\\n",
    "         .withColumn(\"sessStartDate\",f.min('eventTime').over(w_sess))\\\n",
    "         .withColumn(\"sessEndDate\",f.max('eventTime').over(w_sess))\n",
    "\n",
    "df_w.select(sel_cols).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Compute statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task #2:\n",
    "Compute the following statistics:\n",
    "-\tFor each category find median session duration\n",
    "-\tFor each category find # of unique users spending less than 1 min, 1 to 5 mins and more than 5 mins\n",
    "-\tFor each category find top 10 products ranked by time spent by users on product pages - this may require different type of sessions. For this particular task, session lasts until the user is looking at particular product. When particular user switches to another product the new session starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Median session duration for each category ####################\n",
      "+-------------+-----------------+----------------------+---------------+\n",
      "|     category|sessDurMedianMins|sessDurAproxMedianMins|sessDurMeanMins|\n",
      "+-------------+-----------------+----------------------+---------------+\n",
      "|        books|              5.0|                   4.0|            5.0|\n",
      "|    notebooks|              0.5|                   0.0|           0.75|\n",
      "|mobile phones|              3.5|                   0.0|            3.5|\n",
      "+-------------+-----------------+----------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1 For each category find median session duration\n",
    "\n",
    "# due to requirment session lasts per category hovever in majority of cases real world sessions last for\n",
    "# entire user web page visit not category page only.So sessionId grouping by category as well just to \n",
    "# avoid problems with real sessions\n",
    "\n",
    "print(\"#################### Median session duration for each category ####################\")\n",
    "spark.sql(\n",
    "    \"\"\" SELECT  category,\n",
    "                PERCENTILE(sessDurat,0.5) as sessDurMedianMins,\n",
    "                PERCENTILE_APPROX(sessDurat,0.5) as sessDurAproxMedianMins,\n",
    "                MEAN(sessDurat) as sessDurMeanMins\n",
    "        FROM (\n",
    "             SELECT sessId, category,\n",
    "                    FIRST(sessStartDate) as sessStartDate,\n",
    "                    FIRST(sessEndDate) as sessEndDate,\n",
    "                    (CAST(FIRST(sessEndDate) AS LONG)-CAST(FIRST(sessStartDate) AS LONG))/60 as sessDurat\n",
    "             FROM visits_with_sess\n",
    "             GROUP BY sessId,category\n",
    "        )\n",
    "        GROUP BY category\n",
    "    \"\"\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################################\n",
      "# of unique users spending less than 1 min, 1 to 5 mins and more than 5 mins For each category\n",
      "######################################################################################\n",
      "# of unique users spending less than 1 min\n",
      "+-------------+------------+\n",
      "|     category|NofUsrLess1M|\n",
      "+-------------+------------+\n",
      "|    notebooks|           1|\n",
      "|mobile phones|           1|\n",
      "+-------------+------------+\n",
      "\n",
      "# of unique users spending more than 5 mins\n",
      "+-------------+------------+\n",
      "|     category|NofUsrMore5M|\n",
      "+-------------+------------+\n",
      "|        books|           1|\n",
      "|mobile phones|           1|\n",
      "+-------------+------------+\n",
      "\n",
      "# of unique users spending 1 to 5 mins\n",
      "+---------+-----------+\n",
      "| category|NofUsr1to5M|\n",
      "+---------+-----------+\n",
      "|    books|          1|\n",
      "|notebooks|          2|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.2 For each category find # of unique users spending less than 1 min, 1 to 5 mins and more than 5 mins\n",
    "print(\"######################################################################################\")\n",
    "print(\"# of unique users spending less than 1 min, 1 to 5 mins and more than 5 mins For each category\")\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT  sessId, category, userId,\n",
    "                    FIRST(sessStartDate) as sessStartDate,\n",
    "                    FIRST(sessEndDate) as sessEndDate,\n",
    "                    (CAST(FIRST(sessEndDate) AS LONG)-CAST(FIRST(sessStartDate) AS LONG))/60 as sessDurat\n",
    "            FROM visits_with_sess\n",
    "            GROUP BY sessId ,category ,userId\n",
    "\"\"\").createOrReplaceTempView(\"category_per_session\")\n",
    "\n",
    "print(\"# of unique users spending less than 1 min\")\n",
    "spark.sql(\"\"\"SELECT  category,\n",
    "                     COUNT(DISTINCT userId) as NofUsrLess1M\n",
    "            FROM category_per_session\n",
    "            WHERE sessDurat<1\n",
    "            GROUP BY category\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"# of unique users spending more than 5 mins\")\n",
    "spark.sql(\"\"\"SELECT  category,\n",
    "                     COUNT(DISTINCT userId) as NofUsrMore5M\n",
    "            FROM category_per_session\n",
    "            WHERE sessDurat > 5\n",
    "            GROUP BY category\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"# of unique users spending 1 to 5 mins\")\n",
    "spark.sql(\"\"\"SELECT  category,\n",
    "                    COUNT(DISTINCT userId) as NofUsr1to5M\n",
    "             FROM category_per_session\n",
    "             WHERE sessDurat BETWEEN 1 AND 5\n",
    "             GROUP BY category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+-----------+-------------------+-------------------+-------------+----+\n",
      "|  userId|     category|             product| prodSessId|  prodSessStartDate|    prodSessEndDate|prodSessDurat|topN|\n",
      "+--------+-------------+--------------------+-----------+-------------------+-------------------+-------------+----+\n",
      "|user 100|        books|    Java for Dummies|-1158420428|2018-03-01 12:02:00|2018-03-01 12:02:00|          0.0|   1|\n",
      "|user 200|        books|    Romeo and Juliet| 1279730207|2018-03-01 12:15:00|2018-03-01 12:15:00|          0.0|   1|\n",
      "|user 100|    notebooks|      MacBook Pro 15|  653119526|2018-03-01 12:15:00|2018-03-01 12:16:00|          1.0|   1|\n",
      "|user 100|        books|   Scala for Dummies| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|          6.0|   1|\n",
      "|user 200|        books|Sherlock Holmes, ...| 1026034203|2018-03-01 12:11:00|2018-03-01 12:13:00|          2.0|   1|\n",
      "|user 100|mobile phones|       iPhone 8 Plus|  124119555|2018-03-01 12:06:00|2018-03-01 12:07:00|          1.0|   1|\n",
      "|user 300|mobile phones|            iPhone X|  197472950|2018-03-01 12:05:00|2018-03-01 12:05:00|          0.0|   1|\n",
      "|user 100|mobile phones|            iPhone 8| -664156238|2018-03-01 12:08:00|2018-03-01 12:13:00|          5.0|   1|\n",
      "|user 300|    notebooks|         MacBook Air|-2038065633|2018-03-01 12:14:00|2018-03-01 12:20:00|          6.0|   1|\n",
      "|user 200|    notebooks|      MacBook Pro 13| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|          2.0|   1|\n",
      "+--------+-------------+--------------------+-----------+-------------------+-------------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3 For each category find top 10 products ranked by time spent by users on product pages - this may \n",
    "#     require different type of sessions. For this particular task, session lasts until the user is looking\n",
    "#     at particular product. When particular user switches to another product the new session starts.\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"SELECT *,\n",
    "           MIN(eventTime) OVER (PARTITION BY prodSessId) as prodSessStartDate,\n",
    "           MAX(eventTime) OVER (PARTITION BY prodSessId) as prodSessEndDate\n",
    "   FROM (\n",
    "       SELECT userId, category ,product ,eventTime, \n",
    "              IF(\n",
    "                  IFNULL(LAG(product) OVER (PARTITION BY userId, product ORDER BY eventTime),product) = product ,\n",
    "                  FIRST(HASH(CONCAT(userId,category,eventType,eventTime))) OVER (PARTITION BY userId, product ORDER BY eventTime),\n",
    "                  HASH(CONCAT(userId,category,eventType,eventTime))\n",
    "                ) as prodSessId\n",
    "       FROM visits\n",
    "    )\n",
    "\"\"\").createOrReplaceTempView(\"visits_with_sess_per_prod\")\n",
    "\n",
    "# row_N = 1 for removing duplicates . Used instead of group by and manualy selecting all FIRST values \n",
    "spark.sql(\"\"\"SELECT userId ,category ,product ,prodSessId ,prodSessStartDate ,prodSessEndDate , prodSessDurat\n",
    "             FROM (\n",
    "                 SELECT *,\n",
    "                         ROW_NUMBER() OVER (PARTITION BY prodSessId ORDER BY product) as row_N  ,\n",
    "                         (CAST(prodSessEndDate AS LONG)-CAST(prodSessStartDate AS LONG))/60 as prodSessDurat\n",
    "                 FROM visits_with_sess_per_prod\n",
    "             )\n",
    "             where row_N = 1\n",
    "\"\"\").createOrReplaceTempView(\"visits_with_sess_per_prod\")\n",
    "# spark.sql(\"SELECT * FROM visits_with_sess_per_prod\").show(30)\n",
    "\n",
    "spark.sql(\"\"\" SELECT *\n",
    "            FROM(\n",
    "                SELECT *,\n",
    "                        ROW_NUMBER() OVER (PARTITION BY category, product ORDER BY prodSessDurat DESC) as topN\n",
    "                 FROM visits_with_sess_per_prod\n",
    "            )\n",
    "            WHERE topN < 10\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+-------------------+-------------------+---------+\n",
      "|  userId|     category|     sessId|      sessStartDate|        sessEndDate|sessDurat|\n",
      "+--------+-------------+-----------+-------------------+-------------------+---------+\n",
      "|user 100|        books| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|      6.0|\n",
      "|user 200|    notebooks| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|      2.0|\n",
      "|user 200|        books| 1026034203|2018-03-01 12:11:00|2018-03-01 12:15:00|      4.0|\n",
      "|user 300|    notebooks|-2038065633|2018-03-01 12:14:00|2018-03-01 12:14:00|      0.0|\n",
      "|user 100|mobile phones|  124119555|2018-03-01 12:06:00|2018-03-01 12:13:00|      7.0|\n",
      "|user 100|    notebooks|  653119526|2018-03-01 12:15:00|2018-03-01 12:16:00|      1.0|\n",
      "|user 300|mobile phones|  197472950|2018-03-01 12:05:00|2018-03-01 12:05:00|      0.0|\n",
      "|user 300|    notebooks| 1939065348|2018-03-01 12:20:00|2018-03-01 12:20:00|      0.0|\n",
      "+--------+-------------+-----------+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Spark Aggregations\n",
    "w_sess = Window.partitionBy(f.col('sessId')).orderBy(f.col(\"sessId\"))\n",
    "         \n",
    "df_w = df_w.withColumn(\"sessDurat\",(f.col(\"sessEndDate\").cast(\"long\") - f.col(\"sessStartDate\").cast(\"long\"))/60)\n",
    "\n",
    "# df_w.select(['userId','category',  'eventTime','sessId','sessStartDate','sessEndDate','sessDurat']).show(10)\n",
    "df_unic = df_w.select(['userId','category','sessId','sessStartDate','sessEndDate','sessDurat']).dropDuplicates()\n",
    "df_unic.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|     category|sessDurMedian|\n",
      "+-------------+-------------+\n",
      "|        books|          4.0|\n",
      "|    notebooks|          0.0|\n",
      "|mobile phones|          0.0|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1 For each category find median session duration\n",
    "df_unic.groupBy('category').agg(f.expr('percentile_approx(sessDurat, 0.5)').alias('sessDurMedian')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|     category|NofUniqUserLess1M|\n",
      "+-------------+-----------------+\n",
      "|    notebooks|                1|\n",
      "|mobile phones|                1|\n",
      "+-------------+-----------------+\n",
      "\n",
      "+-------------+-----------------+\n",
      "|     category|NofUniqUserMore5M|\n",
      "+-------------+-----------------+\n",
      "|        books|                1|\n",
      "|mobile phones|                1|\n",
      "+-------------+-----------------+\n",
      "\n",
      "+---------+----------------+\n",
      "| category|NofUniqUser1to5M|\n",
      "+---------+----------------+\n",
      "|    books|               1|\n",
      "|notebooks|               2|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2 For each category find # of unique users spending less than 1 min, 1 to 5 mins and more than 5 mins\n",
    "\n",
    "df_unic.where(f.col('sessDurat') < 1)\\\n",
    "        .groupBy('category')\\\n",
    "        .agg(f.countDistinct('userId').alias('NofUniqUserLess1M'))\\\n",
    "        .show()\n",
    "\n",
    "df_unic.where(f.col('sessDurat') > 5)\\\n",
    "        .groupBy('category')\\\n",
    "        .agg(f.countDistinct('userId').alias('NofUniqUserMore5M'))\\\n",
    "        .show()\n",
    "\n",
    "df_unic.where(f.col('sessDurat').between(1,5))\\\n",
    "        .groupBy('category')\\\n",
    "        .agg(f.countDistinct('userId').alias('NofUniqUser1to5M'))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+-----------+-------------------+-------------------+-------------+----+\n",
      "|  userId|     category|             product|     sessId|  sessProdStartDate|    sessProdEndDate|sessProdDurat|rank|\n",
      "+--------+-------------+--------------------+-----------+-------------------+-------------------+-------------+----+\n",
      "|user 100|        books|    Java for Dummies|-1158420428|2018-03-01 12:02:00|2018-03-01 12:02:00|          0.0|   1|\n",
      "|user 200|        books|    Romeo and Juliet| 1279730207|2018-03-01 12:15:00|2018-03-01 12:15:00|          0.0|   1|\n",
      "|user 100|    notebooks|      MacBook Pro 15|  653119526|2018-03-01 12:15:00|2018-03-01 12:16:00|          1.0|   1|\n",
      "|user 100|        books|   Scala for Dummies| -575256446|2018-03-01 12:00:00|2018-03-01 12:06:00|          6.0|   1|\n",
      "|user 200|        books|Sherlock Holmes, ...| 1026034203|2018-03-01 12:11:00|2018-03-01 12:13:00|          2.0|   1|\n",
      "|user 100|mobile phones|       iPhone 8 Plus|  124119555|2018-03-01 12:06:00|2018-03-01 12:07:00|          1.0|   1|\n",
      "|user 300|mobile phones|            iPhone X|  197472950|2018-03-01 12:05:00|2018-03-01 12:05:00|          0.0|   1|\n",
      "|user 100|mobile phones|            iPhone 8| -664156238|2018-03-01 12:08:00|2018-03-01 12:13:00|          5.0|   1|\n",
      "|user 300|    notebooks|         MacBook Air|-2038065633|2018-03-01 12:14:00|2018-03-01 12:20:00|          6.0|   1|\n",
      "|user 200|    notebooks|      MacBook Pro 13| -215346863|2018-03-01 12:15:00|2018-03-01 12:17:00|          2.0|   1|\n",
      "+--------+-------------+--------------------+-----------+-------------------+-------------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3 For each category find top 10 products ranked by time spent by users on product pages -\n",
    "#     this may require different type of sessions. For this particular task, session lasts until \n",
    "#     the user is looking at particular product. When particular user switches to another product the new session starts.\n",
    "\n",
    "sel_cols = ['userId', 'category', 'product','sessId','sessProdStartDate','sessProdEndDate','sessProdDurat']\n",
    "w_prod = Window.partitionBy(f.col('userId'),f.col('product')).orderBy(f.col(\"eventTime\"))\n",
    "w_prod_sess = Window.partitionBy(f.col('sessId'))\n",
    "\n",
    "df_prod = df.withColumn('prevProd', f.lag('product').over(w_prod))\\\n",
    "         .withColumn('hash',f.hash(f.concat(f.col('userId'),f.col('category'),f.col('eventType'),f.col('eventTime'))))\\\n",
    "         .withColumn('sessId',f.when(\n",
    "                                    f.col('prevProd')==f.col('product'),\n",
    "                                    f.first('hash').over(w_prod))\n",
    "                                .otherwise(f.col('hash')))\\\n",
    "         .withColumn(\"sessProdStartDate\",f.min('eventTime').over(w_prod_sess))\\\n",
    "         .withColumn(\"sessProdEndDate\",f.max('eventTime').over(w_prod_sess))\\\n",
    "         .withColumn(\"sessProdDurat\",\\\n",
    "                         (f.col('sessProdEndDate').cast('long')-f.col('sessProdStartDate').cast('long'))/60)\\\n",
    "\n",
    "#leave only uniq sessions not each event \n",
    "df_prod_uniq = df_prod.select(sel_cols).dropDuplicates()\n",
    "\n",
    "#select top_N products in category by session time\n",
    "top_N = 10\n",
    "w_prod_top = Window.partitionBy('category','product').orderBy(f.desc('sessProdDurat'))\n",
    "df_prod_uniq.withColumn('rank',f.row_number().over(w_prod_top)).where(f.col('rank')<top_N).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
